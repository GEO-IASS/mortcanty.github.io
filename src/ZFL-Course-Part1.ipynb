{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Analysis of Multi-Spectral and  Polarimetric SAR Imagery<br /> with Open Source Tools\n",
    "\n",
    "## Mort Canty (mort.canty@gmail.com)\n",
    "\n",
    "## ZFL Bonn, April 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course Outline\n",
    "\n",
    "1. Quick tour of the IPython Notebook\n",
    "\n",
    "2. Mutlispectral visual/infrared images\n",
    "    1. Probability distributions\n",
    "    \n",
    "    2. Principal components\n",
    "    \n",
    "    3. Clustering\n",
    "       \n",
    "3. SAR and polarimetric SAR images\n",
    "    1. Data acquisition and multilooking\n",
    "\n",
    "    1. Probability distributions\n",
    "    \n",
    "    2. Speckle filtering\n",
    "\n",
    "4. Multispecral change detection and radiometric normalization\n",
    "\n",
    "5. Polarimetric SAR change detection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software and Notebook Installation on Windows\n",
    "\n",
    " 1. Install the Docker Engine from https://docs.docker.com/\n",
    "     1. Ensure that hardware virtualization is enabled.\n",
    "     \n",
    "     2. Download DockerToobox from https://www.docker.com/docker-toolboxand <br /> and install with all defaults. Be sure to accept all Oracle drivers.\n",
    "  \n",
    " 2. In the Docker Quickstart terminal, run the commands<br />\n",
    "      __docker pull mort/zfldocker__    \n",
    "      __docker pull mort/datadocker__<br />\n",
    " \t  __docker run -d -v /imagery --name=data mort/datadocker__    \n",
    "      __docker run -d -p 463:8888 --name=zfl --volumes-from=/data mort/zfldocker__   \n",
    "      \n",
    " 3. Run the command<br />\n",
    "      __docker-machine ip default__ <br />\n",
    "    to get the IP address of your virtual machine.  \n",
    "         \n",
    " 3. Point your browser to <br \\>\n",
    "    __http://ip-address:463__\n",
    "    \n",
    " 4. Click on the course notebook __/home/imagery/ZFL-Course.ipynb__, <br />\n",
    "     or open a new notebook with __New/Python 2__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Material\n",
    "\n",
    "<em><b>\n",
    "<a href=\"http://www.amazon.com/Analysis-Classification-Change-Detection-Sensing/dp/1466570377/ref=dp_ob_title_bk\">\n",
    "Canty (2014): Image Analysis, Classification and Change Detection in Remote Sensing, <br />With Algorithms for ENVI/IDL and Python, Third Revised Edition</a> </b>\n",
    "</em>\n",
    "\n",
    "<em><b>\n",
    "<a href=\"http://www.amazon.de/Remote-Sensing-Imaging-Radar-published/dp/B017TH4RXC/ref=sr_1_3?ie=UTF8&qid=1452331615&sr=8-3&keywords=richards+remote+sensing\">\n",
    "Richards (2009): Remote Sensing with Imaging Radar</a> </b>\n",
    "</em>\n",
    "\n",
    "<em><b>\n",
    "<a href=\"http://www.amazon.de/IPython-Interactive-Computing-Visualization-Cookbook/dp/1783284811/ref=sr_1_1?ie=UTF8&qid=1452331905&sr=8-1&keywords=ipython+cookbook\">\n",
    "Rossant (2014): IPython Interactive Computing and Visualization Cookbook</a> </b>\n",
    "</em>\n",
    "\n",
    "<em><b>\n",
    "<a href=\"http://www.amazon.de/Python-Geospatial-Development-Erik-Westra/dp/1849511543/ref=sr_1_2?ie=UTF8&qid=1452332069&sr=8-2&keywords=westra+python\">\n",
    "Westra (2010): Python Geospatial Development</a> </b>\n",
    "</em>\n",
    "\n",
    "<em><b>\n",
    "<a href=\"http://ipython.readthedocs.org/en/stable/index.html\">\n",
    "IPython documentation</a> </b>\n",
    "</em>\n",
    "\n",
    "<em><b>\n",
    "<a href=\"http://trac.osgeo.org/gdal/wiki/GdalOgrInPython\">\n",
    "GDAL documentation</a> </b>\n",
    "</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. IPython Notebook\n",
    "### <span style=\"color:red;\">We are in a Linux environment</span>\n",
    "\n",
    "OS commands are prefixed with __!__ (the __bang__).\n",
    "\n",
    "The __/home__ directory contains the programs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls -l /home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __/home/imagery__ directory contains the image data and this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls -l /home/imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __/home/figures__ directory contains some figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls -l /home/figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "which can be displayed within a __markdown__ cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAR imaging geometry\n",
    "\n",
    "<img src='../imagery/figures/sar_imaging.png' width = '400' height = '400' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can talk directly to the Python kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.rand(5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable graphics output, we run an IPython \"magic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(0,10,100)\n",
    "plt.plot(x,np.sin(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Python programs (scripts) are called with the __run__ command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run /home/dispms.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Multispectral Visual/Infrared Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Scalar (one-band) images\n",
    "\n",
    "The statistical distributions of optical and SAR measurements are fundamentally different: SAR intensity observations are exponentially distributed, optical/infrared observations are normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../imagery/figures/histograms.png' width = '800' height = '800' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *normal* or *Gaussian*  density function of a random variable $Z$ representing an image pixel is\n",
    "\n",
    "$$\n",
    "p(z) = {1\\over \\sqrt{2\\pi}\\sigma}\\exp\\left(-{1\\over 2\\sigma^2}(z-\\mu)^2\\right),\\quad\n",
    "$$\n",
    "\n",
    "where $-\\infty<\\mu<\\infty$ and $\\sigma^2>0$.\n",
    "The mean value and variance are given by\n",
    "\n",
    "$$\n",
    "\\langle Z\\rangle = \\int_{-\\infty}^\\infty z\\cdot p(z) dz = \\mu,\\quad\n",
    "$$\n",
    "$$\n",
    "{\\rm var}(Z) = \\langle(Z-\\langle Z\\rangle)^2\\rangle =  \\int_{-\\infty}^\\infty (z-\\langle Z\\rangle)^2 p(z) dz = \\sigma^2.\\quad\n",
    "$$\n",
    "\n",
    "This is commonly abbreviated by writing\n",
    "\n",
    "$$\n",
    "Z \\sim \\mathcal{N}(\\mu,\\sigma^2).\\quad\n",
    "$$\n",
    "\n",
    "Here is a plot of the normal distribution for $\\mu=0$ and $\\sigma=1$ (the *standard normal* distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "z = np.linspace(-5,5,2000)\n",
    "plt.plot(z,st.norm.pdf(z))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Parameter estimation\n",
    "\n",
    "Consider a multispectral image and a specific land\n",
    "cover category within it. We might choose  $n$ pixels $Z_i$, $i=1\\dots m$,\n",
    "belonging to that category and use them to estimate the moments of\n",
    "the underlying distribution. That distribution will be determined\n",
    "not only by measurement noise, atmospheric disturbances, etc., but\n",
    "also by the spread in reflectances characterizing the land cover\n",
    "category itself.\n",
    "\n",
    "We can estimate the mean and variance of the normal distribution with two *sample functions*:\n",
    "The *sample mean*\n",
    "\n",
    "$$\n",
    "\\bar{Z} = {1\\over m}\\sum_{i=1}^m  Z_i\\quad\n",
    "$$\n",
    "\n",
    "and the *sample variance*\n",
    "\n",
    "$$\n",
    " S = {1\\over m-1}\\sum_{i=1}^m(Z_i - \\bar Z )^2.\\quad\n",
    "$$\n",
    "\n",
    "These two sample functions are also called *unbiased estimators* because\n",
    "their average values are equal to the corresponding moments of the normal distribution, that is,\n",
    "\n",
    "$$\n",
    "\\langle\\bar Z\\rangle = \\langle Z\\rangle \\quad\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\langle S \\rangle = {\\rm var}(Z). \\quad\n",
    "$$\n",
    "\n",
    "### Distributions of the sample mean and variance\n",
    "\n",
    "The sample mean $\\bar Z$ is normally distributed with mean $\\mu$ and variance $\\sigma^2/m$. \n",
    "\n",
    "$$\n",
    "p(z) = {1\\over \\sqrt{2\\pi\\over m}\\sigma}\\exp\\left(-{1\\over 2(\\sigma^2/m)}(z-\\mu)^2\\right),\\quad\n",
    "$$\n",
    "\n",
    "\n",
    "For the sample variance $S$ then\n",
    "\n",
    "$$\n",
    " (m-1)S/\\sigma^2 \\quad\n",
    "$$\n",
    "\n",
    "is independent of $\\bar Z$ and has the *chi-square* distribution with $m-1$ degrees of freedom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gamma distribution and its relatives\n",
    "\n",
    "A random variable $Z$ is said to have a *gamma distribution* if its probability\n",
    "density function is given by\n",
    "\n",
    "$$\n",
    "p_\\gamma(z) = \\cases{{1\\over \\beta^\\alpha\\Gamma(\\alpha)}z^{\\alpha-1}e^{-z/\\beta} & for $z>0$\\cr\n",
    "            0 & elsewhere,} \\quad\n",
    "$$\n",
    "\n",
    "where $\\alpha>0$ and $\\beta>0$ and where the *gamma function* $\\Gamma(\\alpha)$ is given by\n",
    "\n",
    "$$\n",
    "\\Gamma(\\alpha) = \\int_0^\\infty z^{\\alpha-1} e^{-z}dz,\\quad \\alpha>0. \\quad\n",
    "$$\n",
    "\n",
    "The gamma function has the recursive property\n",
    "\n",
    "$$\n",
    "\\Gamma(\\alpha) = (\\alpha-1)\\Gamma(\\alpha-1),\\quad \\alpha>1, \\quad\n",
    "$$\n",
    "\n",
    "and generalizes the notion of a factorial. The gamma distribution has mean  and\n",
    "variance\n",
    "\n",
    "$$\n",
    "\\langle Z\\rangle = \\alpha\\beta, \\quad {\\rm var}(Z) = \\alpha\\beta^2. \\quad\n",
    "$$\n",
    "\n",
    "A  special case of the gamma distribution arises\n",
    "for $\\alpha = 1$. Since $\\Gamma(1) = \\int_0^\\infty e^{-z}dz = 1$,\n",
    "we obtain the *exponential distribution* with density function\n",
    "\n",
    "$$\n",
    "p_e(z)  = \\cases{{1\\over \\beta}e^{-z/\\beta} & for $z>0$\\cr \\quad\n",
    "            0 & elsewhere,}\n",
    "$$\n",
    "\n",
    "where $\\beta >0$. The exponential distribution has mean $\\beta$ and variance $\\beta^2$. In addition\n",
    "If random variables $Z_1,Z_2\\dots Z_m$ are independent and\n",
    "exponentially distributed, then \n",
    "\n",
    "$$\n",
    "Z = \\sum_{i=1}^m Z_i \\quad\n",
    "$$\n",
    "\n",
    "is gamma distributed with $\\alpha = m$.\n",
    "\n",
    "The *chi-square distribution with $m$ degrees of freedom* is another special case of the gamma distribution.\n",
    "We get its density function with $\\beta = 2$ and $\\alpha = m/2$, i.e.,\n",
    "\n",
    "$$\n",
    "p_{\\chi^2;m}(z)=\\cases{{1 \\over 2^{m/2}\\Gamma(m/2)} z^{(m-2)/2} e^{-z/2} & for $z>0$\\cr 0 & otherwise.} \\quad\n",
    "$$\n",
    "\n",
    "It follows that the chi-square distribution has mean $\\mu = m$ and variance\n",
    "$\\sigma^2 = 2m$. \n",
    "\n",
    "Here are plots of the chi-square distribution for different degrees of freedom:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = np.linspace(1,40,200)\n",
    "for m in [1,2,3,4,5,10,20]:\n",
    "    plt.plot(z,st.chi2.pdf(z,m))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The gamma and exponential distributions will be essential for the characterization of\n",
    "SAR speckle noise in SAR imagery. The chi-square distribution plays a central role in the iterative change detection algorithm IR-MAD that we will be looking at later.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Vector (multispectral or N-band) images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A measured pixel or observation in an N-band multispectral image correspond to a *random vector*\n",
    "\n",
    "$$\n",
    "Z = \\pmatrix{Z_1\\cr \\vdots  \\cr Z_N} \\quad\n",
    "$$\n",
    "\n",
    "i.e., a vector the components of which are normally distributed random variables.\n",
    "\n",
    "The *covariance* of two random variables $Z_1$ and $Z_2$ is a measure of how their realizations\n",
    "are dependent upon each other and is defined as\n",
    "\n",
    "$$\n",
    "{\\rm cov}(Z_1,Z_2) = \\left\\langle\\ (Z_1-\\langle Z_1\\rangle)(Z_2-\\langle Z_2\\rangle)\\ \\right\\rangle.\\quad\n",
    "$$\n",
    "\n",
    "The random vector $Z$ has a *multivariate normal* density function when\n",
    "\n",
    "$$\n",
    "p(z) = {1\\over (2\\pi)^{N/2}\\sqrt{|\\Sigma|}}\n",
    "\\exp\\left(-{1\\over 2}(z-\\mu)^\\top \\Sigma^{-1} (z-\\mu)\\right), \\quad\n",
    "$$\n",
    "\n",
    "The mean vector is \n",
    "\n",
    "$$\n",
    "\\langle Z\\rangle = \\mu \\quad\n",
    "$$ \n",
    "\n",
    "and $\\Sigma$ is the *covariance matrix*. This is indicated by writing\n",
    "\n",
    "$$\n",
    " Z \\sim \\mathcal{N}(\\mu,\\Sigma).\\quad\n",
    "$$\n",
    "\n",
    "The covariance matrix is a convenient way of representing the variances and covariances of the components\n",
    "of a random vector. Let $a = (a_1,a_2)^\\top$\n",
    "be any constant vector. Then the variance of the random variable\n",
    "$ a^\\top Z = a_1Z_1+a_2Z_2$ is\n",
    "\n",
    "$$\n",
    "{\\rm var}(a^\\top Z) = {\\rm cov}(a_1Z_1+a_2Z_2,a_1Z_1+a_2Z_2) \\\\\n",
    "=a_1^2{\\rm var}(Z_1)+a_1a_2{\\rm cov}(Z_1,Z_2)+a_1a_2{\\rm cov}(Z_2,Z_1)+a_2^2{\\rm var}(Z_2)\\\\\n",
    "=(a_1,a_2) \\pmatrix{{\\rm var}(Z_1)&{\\rm cov}(Z_1,Z_2)\\cr{\\rm cov}(Z_2,Z_1)&{\\rm var}(Z_2)}\n",
    " \\pmatrix{a_1\\cr a_2} \\\\\n",
    " = a^\\top\\Sigma a.\n",
    "$$\n",
    "\n",
    "A *complex Gaussian random variable* $Z = X + i Y$ is a complex random variable whose real\n",
    "and imaginary parts are bivariate normally distributed.\n",
    "Under certain assumptions, the real-valued form for the multivariate distribution\n",
    "can be carried over straightforwardly to the domain of complex random vectors.\n",
    "In particular, it is assumed that the real and imaginary parts of each component of the complex\n",
    "random vector are uncorrelated and have equal variances, although the real and imaginary parts of different components can be\n",
    "correlated. As we shall see later, this corresponds closely to the properties\n",
    "of  SAR amplitude data. The complex covariance matrix for a zero-mean complex random vector $Z$ is\n",
    "given by\n",
    "\n",
    "$$\n",
    "\\Sigma=\\langle Z Z^+\\rangle,\n",
    "$$\n",
    "\n",
    "where the $^+$ denotes conjugate transposition. \n",
    "The complex random vector $Z$ is said to be *complex multivariate normally distributed* with zero mean\n",
    "and covariance matrix $\\Sigma$ if its density function is\n",
    "\n",
    "$$\n",
    "p(z) = {1\\over \\pi^N|\\Sigma|}\\exp(-z^+\\Sigma^{-1} z).\n",
    "$$\n",
    "\n",
    "This is indicated by writing \n",
    "\n",
    "$$\n",
    "Z \\sim \\mathcal{N}_C(0,\\Sigma).\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter estimation and the data matrix\n",
    "\n",
    "The {\\it vector sample mean} is given by\n",
    "\n",
    "$$\n",
    "\\bar{Z} = {1\\over m}\\sum_{\\nu=1}^m  Z(\\nu)\n",
    "$$\n",
    "\n",
    "and the *sample covariance matrix* by\n",
    "\n",
    "$$\n",
    " S = {1\\over m-1}\\sum_{\\nu=1}^m( Z(\\nu) - \\bar Z ) (Z(\\nu) - \\bar Z)^\\top.\n",
    "$$\n",
    "\n",
    "These two sample functions are unbiased estimators because again, as in the scalar case,\n",
    "their expected values are equal to the corresponding parameters of $P(z)$, that is,\n",
    "\n",
    "$$\n",
    "\\langle\\bar{Z}\\rangle = \\langle Z\\rangle\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\langle S \\rangle = \\Sigma.\n",
    "$$\n",
    "\n",
    "Suppose that we have made observations $z(\\nu)$, $\\nu=1\\dots m$. They may be  arranged\n",
    "into an $m\\times N$ matrix $\\tilde Z$ in which each $N$-component observation vector\n",
    "forms a row, i.e.,\n",
    "\n",
    "$$\n",
    "\\tilde Z  = \\pmatrix{z(1)^\\top \\cr z(2)^\\top\\cr\\vdots\\cr z(m)^\\top},\n",
    "$$\n",
    "\n",
    "which is a very useful construct, and is called the *data matrix*.\n",
    "\n",
    "The unbiased estimate $\\bar z$ of the sample mean vector $\\bar Z$ is just the vector of the column means of the data matrix. It can be conveniently written as\n",
    "\n",
    "$$\n",
    "\\bar z = {1\\over m}\\tilde Z^\\top 1_m,\n",
    "$$\n",
    "\n",
    "where $1_m$ denotes a  column vector of $m$ ones.\n",
    "If the column means have been subtracted out, then the data matrix\n",
    "is said to be *column centered*, in which case\n",
    "an unbiased estimate $s$ for the covariance matrix $\\Sigma$ is given by\n",
    "\n",
    "$$\n",
    "s = {1\\over m-1}\\tilde Z^\\top\\tilde Z.\n",
    "$$\n",
    "\n",
    "The rules of matrix multiplication take care of the sum over all observations, so it only remains\n",
    "to divide by $m-1$ to obtain the desired estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Principal components\n",
    "\n",
    "The principal components transformation, also called *principal components analysis* (PCA),\n",
    "generates linear combinations of multi-spectral\n",
    "pixel intensities which are mutually uncorrelated and which have maximum variance.\n",
    "\n",
    "Consider a multispectral image represented by the random vector $G$ ($G=$ vector of gray-scale values)\n",
    "and assume that $\\langle G\\rangle=0$, so that the covariance matrix \n",
    "is given by \n",
    "\n",
    "$$ \n",
    "S=\\langle G G^\\top\\rangle.\n",
    "$$ \n",
    "\n",
    "Let us seek a linear combination \n",
    "\n",
    "$$\n",
    "Y =  w^\\top G\n",
    "$$\n",
    "\n",
    "whose variance $w^\\top \\Sigma w$ is maximum. This\n",
    "quantity can trivially be made as large as we like by choosing $w$ sufficiently\n",
    "large, so that the maximization only makes sense if we restrict $w$ in some way.\n",
    "A convenient constraint is $w^\\top w=1$.\n",
    "We can solve this problem by maximizing the *Lagrange function*\n",
    "\n",
    "$$\n",
    "L =  w^\\top \\Sigma w - \\lambda(w^\\top w - 1).\n",
    "$$\n",
    "\n",
    "This leads directly to the eigenvalue problem\n",
    "\n",
    "$$\n",
    "\\Sigma  w = \\lambda w.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls -l /home/imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run /home/pca /home/imagery/20010525"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run /home/dispms -f /home/imagery/20010525_pca -p [1,2,3] -e 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote the __orthogonal and normalized eigenvectors__ of $\\Sigma$ obtained by\n",
    "solving the above problem by $w_1\\dots w_N$, __sorted according to\n",
    "decreasing eigenvalue__ $\\lambda_1\\ge \\dots\\ge\\lambda_N$. These eigenvectors are the\n",
    "*principal axes* and the corresponding linear combinations $w_i^\\top G$ are projections\n",
    "along the principal axes, called the *principal components* of $ G$. The individual principal components\n",
    "\n",
    "$$\n",
    "Y_1 = w_1^\\top G,\\ Y_2= w_2^\\top G,\\ \\dots,\\ Y_N= w_N^\\top G\n",
    "$$\n",
    "\n",
    "can be expressed more compactly as a random vector $Y$ by writing\n",
    "\n",
    "$$\n",
    "Y =  W^\\top G,\n",
    "$$\n",
    "\n",
    "where $W$ is the  matrix whose columns comprise the eigenvectors, that is,\n",
    "\n",
    "$$\n",
    "W = (w_1\\dots w_N).\n",
    "$$\n",
    "\n",
    "Since the eigenvectors are orthogonal and normalized, $W$ is an *orthonormal matrix*:\n",
    "\n",
    "$$\n",
    "W^\\top  W= I.\n",
    "$$\n",
    "\n",
    "If the covariance matrix of the principal components vector $ Y$ is called $\\Sigma'$, then  we have\n",
    "\n",
    "$$\n",
    "\\Sigma' = \\langle Y Y^\\top \\rangle= \\langle W^\\top  G  G^\\top  W\\rangle\n",
    "$$\n",
    "\n",
    "or, since $\\Sigma w_i = \\lambda_i w_i$,\n",
    "\n",
    "$$\n",
    "\\Sigma'= W^\\top \\Sigma  W =\n",
    "\\pmatrix{\\lambda_1 & 0 & \\cdots &0\\cr\n",
    "                   0 &\\lambda_2& \\cdots&0\\cr\n",
    "                   \\vdots&\\vdots&\\ddots&\\vdots\\cr\n",
    "                   0 & 0& \\cdots&\\lambda_N}\n",
    "$$\n",
    "\n",
    "The eigenvalues are thus seen to be the variances of the principal components, and all of the covariances are\n",
    "zero. The first principal component $Y_1$ has maximum variance ${\\rm var}(Y_1)=\\lambda_1$, the second principal\n",
    "component $Y_2$ has maximum variance ${\\rm var}(Y_2)=\\lambda_2$ subject to the condition that it is uncorrelated\n",
    "with $Y_1$, and so on.\n",
    "The fraction of the total variance in the original multispectral image which\n",
    "is accounted for by the first $i$ principal components is\n",
    "$$\n",
    "{\\lambda_1+\\dots +\\lambda_i\\over\\lambda_1+\\dots +\\lambda_i+\\dots +\\lambda_N}.\n",
    "$$\n",
    "\n",
    "Let's look at the first and last principal components of the image above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run /home/dispms -f /home/imagery/20010525_pca -p [1,1,1] -e 4 -F /home/imagery/20010525_pca -P [6,6,6] -E 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of sample mean vector and sample covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the multivariate case we have a similar situation to the unovariate case. The sample mean vector\n",
    "\n",
    "$$\n",
    "\\bar Z = {1\\over m}\\sum_{\\nu=1}^m  Z(\\nu)\n",
    "$$\n",
    "\n",
    "is multivariate normally distributed with mean $\\mu$ and covariance matrix $\\Sigma/m$ for\n",
    "samples \n",
    "\n",
    "$$\n",
    "Z(\\nu)\\sim\\mathcal{N}(\\mu,\\Sigma),\\quad \\nu = 1\\dots m.\n",
    "$$\n",
    "\n",
    "The sample covariance matrix $S$ is described by a {\\it Wishart distribution}:\n",
    "Suppose \n",
    "\n",
    "$$\n",
    "Z(\\nu)\\sim\\mathcal{N}(0,\\Sigma),\\quad\\nu=1\\dots m.\n",
    "$$\n",
    "\n",
    "Define\n",
    "\n",
    "$$\n",
    "X = (m-1)S = \\sum_{\\nu=1}^m  Z(\\nu) Z(\\nu)^\\top.\n",
    "$$\n",
    "\n",
    "The probability density function of $X$  is\n",
    "the *Wishart density with $m$ degrees of freedom*:\n",
    "\n",
    "$$\n",
    "p_W(x) ={|x|^{(m-N-1)/2}\\exp(-{\\rm tr}(\\Sigma^{-1} x)/2) \\over\n",
    "2^{Nm/2}\\pi^{N(N-1)/4}|\\Sigma|^{m/2}\\prod_{i=1}^N\\Gamma[(m+1-i)/2]}\n",
    "$$\n",
    "\n",
    "for $x$ positive definite, and $0$ otherwise.\n",
    "\n",
    "We write \n",
    "\n",
    "$$\n",
    "X \\sim \\mathcal{W}(\\Sigma,N,m). \n",
    "$$\n",
    "\n",
    "Since the gamma function $\\Gamma(\\alpha)$, is not defined for $\\alpha\\le 0$, the Wishart distribution is undefined\n",
    "for $m<N$. The Wishart density generalizes the chi-square density, as may easily be seen by\n",
    "setting $N\\to 1$  and $\\Sigma\\to 1$.\n",
    "\n",
    "We don't need the Wishart distribution here, but later when we talk about SAR change detection, we will need its analog for complex multinormal random variables representing polarimetric data:\n",
    "\n",
    "$$\n",
    "X = \\sum_{\\nu=1}^m Z(\\nu) Z(\\nu)^+,\n",
    "$$\n",
    "\n",
    "The probability density function of $X$  is\n",
    "the *complex Wishart density with $m$ degrees of freedom*:\n",
    "\n",
    "$$\n",
    "p_{W_c}( x) ={|x|^{(m-N)}\\exp(-{\\rm tr}(\\Sigma^{-1}x)) \\over\n",
    "\\pi^{N(N-1)/2}|\\Sigma|^{m}\\prod_{i=1}^N\\Gamma(m+1-i)}.\n",
    "$$\n",
    "\n",
    "This is denoted \n",
    "\n",
    "$$\n",
    "X\\sim \\mathcal{W}_C(\\Sigma,N,m).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Clustering or unsupervised classification\n",
    "\n",
    "We represent the observed pixels in a multispectral image by the set\n",
    "\n",
    "$$\n",
    "\\{z(\\nu) \\mid \\nu = 1\\dots m\\}.\n",
    "$$\n",
    "\n",
    "A given partitioning or clustering may be written in the form\n",
    "\n",
    "$$\n",
    "C=[C_1,\\dots C_k,\\dots C_K],\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "C_k = \\{\\ \\nu\\mid  z(\\nu)\\hbox{ is in class } k \\}.\n",
    "$$\n",
    "\n",
    "Let $u_{k\\nu}$ be the probability that pixel $\\nu$ is class $C_k$ *given its observed value* $z(\\nu)$\n",
    "\n",
    "$$\n",
    "u_{k\\nu} = {\\rm Pr}(k \\mid z(\\nu))\n",
    "$$\n",
    "\n",
    "This is called the *posterior probability* and can be written, from Bayes' Rule, as\n",
    "\n",
    "$$\n",
    "u_{k\\nu} =  {p(z(\\nu)\\mid k){\\rm Pr}(k)\\over p(z(\\nu))}\n",
    "$$\n",
    "\n",
    "${\\rm Pr}(k)$ is called the *prior probability* for class $C_k$.\n",
    "\n",
    "Now assume that the pixels in each class are (approximately) multivariate normally distributed with mean $\\mu_k$ and covariance matrix $\\Sigma_k$. Then\n",
    "\n",
    "$$\n",
    "u_{k\\nu} = {1\\over \\sqrt{|\\Sigma_k|}}\\exp\\left[-{1\\over 2}(z(\\nu)-\\mu_k)^\\top\\Sigma_k^{-1} (z(\\nu)-\\mu_k)\\right]p_k\\quad (1)\n",
    "$$\n",
    "\n",
    "apart from a normalization constant. In the above expression the prior probability ${\\rm Pr}(k)$ has been replaced by\n",
    "\n",
    "$$\n",
    "p_k = {m_k\\over m} = {1\\over m}\\sum_{\\nu=1}^m u_{k\\nu}. \\quad (2)\n",
    "$$\n",
    "\n",
    "To complete the picture we still need expressions for $\\mu_k$ and $\\Sigma_k$.\n",
    "\n",
    "$$\n",
    "\\mu_k = {1\\over m_k}\\sum_{\\nu\\in C_k}z(\\nu) =\n",
    "{\\sum_{\\nu=1}^m u_{k\\nu}z(\\nu) \\over \\sum_{\\nu=1}^m u_{k\\nu}},\\quad k=1\\dots K, \\quad (3)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_k=\n",
    "{\\sum_{\\nu=1}^m u_{k\\nu}(z(\\nu)-\\mu_k) (z(\\nu)-\\mu_k)^\\top \\over \\sum_{\\nu=1}^m u_{k\\nu}},\\quad k=1\\dots K.\\quad (4)\n",
    "$$\n",
    "\n",
    "#### Algorithm (Expectation maximization clustering)\n",
    "\n",
    "1. Choose random values for the initial class memeberships $u_{k\\nu}$ with $\\sum_{k=1}^K u_{k\\nu}=1$\n",
    "\n",
    "2. Determine the cluster means $\\mu_k$ with Equation (3) , then the covariance matrices $\\Sigma_k$ for each cluster with  Equation (4) and the priors $p_k$ with Equation (2)\n",
    "\n",
    "3. Calculate the new class membeship probabilities $u_{k\\nu}$ with Equation (1), and normalize to ensure that $\\sum_{k=1}^K u_{k\\nu}=1$\n",
    "\n",
    "4. If the $u_{k\\nu}$ values have stopped changing significantly, stop, else go to step 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run /home/em -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run /home/pca /home/imagery/20010525"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run /home/em -p [1,2,3] -K 8 /home/imagery/20010525_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run /home/dispms -c -f /home/imagery/20010525_pca_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
